# ğŸ¬ IMDb Sentiment Analysis Using TextBlob & VADER

This project performs sentiment analysis on 50,000 IMDb movie reviews using two popular NLP tools: **TextBlob** and **VADER**. The goal is to classify reviews as **positive** or **negative**, compare both tools, and experiment with threshold tuning to improve performance.

> âœ… This project was submitted as part of my MSc in Data Science coursework.

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ data/                # IMDb dataset
â”‚
â”œâ”€â”€ notebook/            
â”‚   â”œâ”€â”€ sentiment_analysis.ipynb
â”‚   â””â”€â”€ images/          # Word clouds & confusion matrices
â”‚       â”œâ”€â”€ cm_textblob_0_thr.png
â”‚       â”œâ”€â”€ cm_textblob_0.1_thr.png
â”‚       â”œâ”€â”€ cm_vader_0_thr.png
â”‚       â”œâ”€â”€ cm_vader_0.5_thr.png
â”‚       â””â”€â”€ word_cloud.png
```

---

## ğŸ“Š Dataset

- **Source:** [IMDb Dataset of 50K Movie Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)
- **Size:** 50,000 reviews, evenly split between positive and negative labels
- **Use case:** Binary sentiment classification

---

## ğŸ§¹ Preprocessing

- Removed punctuation, special characters, and stopwords
- Converted text to lowercase
- Tokenized the reviews
- Verified **balanced class distribution** (25k positive, 25k negative)

---

## â˜ï¸ Word Cloud EDA

### Word Cloud - Positive vs Negative Reviews
![wordcloud](notebook/images/word_cloud.png)

Word clouds show frequent words in each sentiment group:
- **Positive reviews** use words like _"great"_, _"love"_, _"story"_, _"performance"_
- **Negative reviews** include terms like _"bad"_, _"worst"_, _"boring"_, _"waste"_

---

## ğŸ§  Sentiment Classification

Î™ analyzed sentiment using both tools:

| Tool      | Default Threshold | Tuned Threshold | Accuracy (default) | Accuracy (tuned) |
|-----------|-------------------|------------------|---------------------|------------------|
| TextBlob  | `> 0`             | `> 0.1`          | 0.70                | **0.76**         |
| VADER     | `> 0`             | `> 0.5`          | 0.68                | **0.70**         |

---

### ğŸ“Œ Threshold Tuning Insights

#### ğŸ”¹ TextBlob Confusion Matrices

### TextBlob Confusion Matrix (Threshold = 0)
![textblob_0](notebook/images/cm_textblob_0_thr.png)

### TextBlob Confusion Matrix (Threshold = 0.1)
![textblob_0.1](notebook/images/cm_textblob_0.1_thr.png)

- At **0 threshold**, many negative reviews are misclassified as positive.
- Raising the threshold to **0.1** significantly reduces false positives, improving accuracy.

#### ğŸ”¹ VADER Confusion Matrices

### VADER Confusion Matrix (Threshold = 0)
![vader_0](notebook/images/cm_vader_0_thr.png)

### VADER Confusion Matrix (Threshold = 0.5)
![vader_0.5](notebook/images/cm_vader_0.5_thr.png)

- At **0 threshold**, VADER favors predicting positives, missing many negatives.
- A stricter threshold of **0.5** balances predictions and improves recall for negatives.

---

## ğŸ“ˆ Evaluation Metrics

We used the following to assess model performance:

- **Accuracy**
- **Precision**
- **Recall**
- **F1-Score**
- **Confusion Matrix**

TextBlob yielded **higher overall accuracy and F1-score**, especially for negative reviews. VADER was slightly better at catching positives (high recall), but less consistent on negatives.

---

## ğŸ§ª Tools & Libraries

- Python, Pandas, Numpy
- TextBlob, VADER
- NLTK (tokenization, stopwords)
- Matplotlib & Seaborn
- WordCloud
- scikit-learn (metrics, confusion matrix)

---

## ğŸ“œ License

MIT License

---

## ğŸ™‹â€â™‚ï¸ Author

ğŸ“§ Dimitris Zografos â€” MSc Data Science  
